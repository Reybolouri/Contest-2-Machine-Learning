# Contest 2 Machine Learning

There are 3 scripts for this contest. 2 CNN models with slight differences and one xgb model.  3L_CNN_script.ipynb is a 3layer CNNmodel that yeilds 98%accuracy ; Both_CNN_scripts.ipynb containd a 2 layer model and 3 layer model that is identical to the previous one. Non-NN_model(xgb).ipynb is the xgboost model.  

For this contest, first I tried a simple 2 layer CNN model, consisting of two convolutional blocks (3→32, 32→64) each with ReLU and max pooling, flattenning the resulting 64×8×8 feature map into a 4096 dimensional vector, and applies a single dropout layer in the fully connected head. Its design makes it fast and lightweight, but without batch normalization or extra layers it can’t learn as deepand well regularized features as a deeper network would. So I moved on to a 3layer CNN. This three layer CNN adds a third convolutional block (with 3→32→64→128 filters), each including batch normalization, activation, pooling, and dropout, to progressively learn richer image features down to a 128×4×4 map. It then flattens these 2048 features into a 128 unit dense layer (with dropout) before the final three way classification.
