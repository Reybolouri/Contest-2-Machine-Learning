{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\reyha\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\reyha\\appdata\\roaming\\python\\python313\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\reyha\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\reyha\\appdata\\roaming\\python\\python313\\site-packages (3.10.0)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install pandas numpy scikit-learn matplotlib seaborn tensorflow keras xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# In your .ipynb, start with necessary imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1200, 3074) Test: (1200, 3073)\n"
     ]
    }
   ],
   "source": [
    "# 1.a. Point to the data folder\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# 1.b. Read in training and test tables\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 2.a. Check for missing values\n",
    "assert train_df.isna().sum().sum() == 0, \"Missing in train!\"\n",
    "assert test_df .isna().sum().sum() == 0, \"Missing in test!\"\n",
    "\n",
    "# 2.b. Separate IDs & labels\n",
    "train_ids = train_df[\"id\"].values\n",
    "train_labels = train_df[\"y\"].values.astype(np.int64) - 1  # zero-based for PyTorch\n",
    "train_df = train_df.drop(columns=[\"id\", \"y\"])\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_df = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "# 2.c. Normalize pixel intensities (already in [0,1], but cast to float32)\n",
    "train_pixels = train_df.values.astype(np.float32)\n",
    "test_pixels  = test_df.values .astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 3.a. Each row is [r0101 … r3132, g0101 …, b0101 …], so reshape to (N, 3, 32, 32)\n",
    "def to_image_array(flat_array):\n",
    "    # shape (N, 3072) → (N, 3, 32, 32)\n",
    "    return flat_array.reshape(-1, 3, 32, 32)\n",
    "\n",
    "X_train = to_image_array(train_pixels)\n",
    "X_test  = to_image_array(test_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FarmImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None):\n",
    "        self.images = torch.from_numpy(images)      # float32 tensor\n",
    "        self.labels = None if labels is None else torch.from_numpy(labels)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]\n",
    "        if self.labels is None:\n",
    "            return x\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Split train into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = FarmImageDataset(X_tr, y_tr)\n",
    "val_ds   = FarmImageDataset(X_val, y_val)\n",
    "test_ds  = FarmImageDataset(X_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # → (32,32,32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # → (32,16,16)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # → (64,16,16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # → (64,8,8)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                                 # → (64*8*8 = 4096)\n",
    "            nn.Linear(64*8*8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 – train_acc: 0.968, val_acc: 0.954\n",
      "Epoch 02 – train_acc: 0.985, val_acc: 0.929\n",
      "Epoch 03 – train_acc: 0.998, val_acc: 0.967\n",
      "Epoch 04 – train_acc: 1.000, val_acc: 0.954\n",
      "Epoch 05 – train_acc: 0.998, val_acc: 0.958\n",
      "Epoch 06 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 07 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 08 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 09 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 10 – train_acc: 1.000, val_acc: 0.963\n",
      "Epoch 11 – train_acc: 0.999, val_acc: 0.958\n",
      "Epoch 12 – train_acc: 1.000, val_acc: 0.954\n",
      "Epoch 13 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 14 – train_acc: 1.000, val_acc: 0.963\n",
      "Epoch 15 – train_acc: 1.000, val_acc: 0.954\n",
      "Epoch 16 – train_acc: 1.000, val_acc: 0.963\n",
      "Epoch 17 – train_acc: 1.000, val_acc: 0.963\n",
      "Epoch 18 – train_acc: 0.999, val_acc: 0.963\n",
      "Epoch 19 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 20 – train_acc: 1.000, val_acc: 0.950\n",
      "Epoch 21 – train_acc: 1.000, val_acc: 0.967\n",
      "Epoch 22 – train_acc: 1.000, val_acc: 0.963\n",
      "Epoch 23 – train_acc: 1.000, val_acc: 0.954\n",
      "Epoch 24 – train_acc: 1.000, val_acc: 0.958\n",
      "Epoch 25 – train_acc: 1.000, val_acc: 0.967\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_correct += (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "def eval_epoch(loader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)\n",
    "            total_loss += criterion(preds, yb).item() * Xb.size(0)\n",
    "            total_correct += (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, 26):\n",
    "    train_loss, train_acc = train_epoch(train_loader)\n",
    "    val_loss, val_acc     = eval_epoch(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_cnn.pth\")\n",
    "    print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_2.csv with 1200 rows.\n"
     ]
    }
   ],
   "source": [
    "# 7.a. Load best model\n",
    "model.load_state_dict(torch.load(\"best_cnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 7.b. Predict on test set\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for Xb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        preds = model(Xb).argmax(1).cpu().numpy() + 1  # back to 1–3 labels\n",
    "        all_preds.append(preds)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "\n",
    "# 7.c. Write submission CSV\n",
    "submission = pd.DataFrame({\"id\": test_ids, \"y\": all_preds})\n",
    "submission.to_csv(\"cnn_2.csv\", index=False)\n",
    "print(\"cnn_2.csv with\", len(submission), \"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 – train_acc: 0.982, val_acc: 0.958\n",
      "Epoch 02 – train_acc: 0.974, val_acc: 0.963\n",
      "Epoch 03 – train_acc: 0.978, val_acc: 0.954\n",
      "Epoch 04 – train_acc: 0.977, val_acc: 0.967\n",
      "Epoch 05 – train_acc: 0.973, val_acc: 0.950\n",
      "Epoch 06 – train_acc: 0.979, val_acc: 0.958\n",
      "Epoch 07 – train_acc: 0.986, val_acc: 0.942\n",
      "Epoch 08 – train_acc: 0.983, val_acc: 0.938\n",
      "Epoch 09 – train_acc: 0.983, val_acc: 0.950\n",
      "Epoch 10 – train_acc: 0.990, val_acc: 0.942\n",
      "Epoch 11 – train_acc: 0.992, val_acc: 0.942\n",
      "Epoch 12 – train_acc: 0.990, val_acc: 0.967\n",
      "Epoch 13 – train_acc: 0.994, val_acc: 0.946\n",
      "Epoch 14 – train_acc: 0.994, val_acc: 0.963\n",
      "Epoch 15 – train_acc: 0.994, val_acc: 0.967\n",
      "Epoch 16 – train_acc: 0.997, val_acc: 0.912\n",
      "Epoch 17 – train_acc: 0.991, val_acc: 0.958\n",
      "Epoch 18 – train_acc: 0.998, val_acc: 0.958\n",
      "Epoch 19 – train_acc: 0.999, val_acc: 0.958\n",
      "Epoch 20 – train_acc: 0.997, val_acc: 0.958\n",
      "\n",
      "Best validation accuracy achieved: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# ... your training loop ...\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, 21):\n",
    "    train_loss, train_acc = train_epoch(train_loader)\n",
    "    val_loss, val_acc     = eval_epoch(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_cnn.pth\")\n",
    "    print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n",
    "\n",
    "# === ADD THIS AT THE END ===\n",
    "print(f\"\\nBest validation accuracy achieved: {best_val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation accuracy (re-loaded best model): 97.08%\n"
     ]
    }
   ],
   "source": [
    "# Load best‐weights and re‐eval\n",
    "model.load_state_dict(torch.load(\"best_cnn.pth\"))\n",
    "val_loss, val_acc = eval_epoch(val_loader)\n",
    "print(f\"Final validation accuracy (re-loaded best model): {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.08750\teval-merror:0.29583\n",
      "[10]\ttrain-merror:0.00104\teval-merror:0.20833\n",
      "[20]\ttrain-merror:0.00104\teval-merror:0.18750\n",
      "[30]\ttrain-merror:0.00000\teval-merror:0.17083\n",
      "[40]\ttrain-merror:0.00000\teval-merror:0.17500\n",
      "XGBoost validation accuracy: 82.92%\n",
      "Saved xgb.csv with 1200 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Train/Val split (reusing train_pixels, train_labels from before)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    train_pixels, train_labels,\n",
    "    test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest  = xgb.DMatrix(test_pixels)\n",
    "\n",
    "# 4. Set parameters\n",
    "params = {\n",
    "    'objective':      'multi:softmax',   # for classification\n",
    "    'num_class':      3,                 # three classes\n",
    "    'eval_metric':    'merror',          # multiclass error rate\n",
    "    'learning_rate':  0.1,\n",
    "    'max_depth':      6,\n",
    "    'subsample':      0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed':           42\n",
    "}\n",
    "\n",
    "# 5. Train with early stopping\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=200,\n",
    "    evals=[(dtrain, 'train'), (dval, 'eval')],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10\n",
    ")\n",
    "\n",
    "# 6. Validation accuracy\n",
    "val_preds = bst.predict(dval).astype(int)   # returns 0,1,2\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"XGBoost validation accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "# 7. Predict on test set and save submission\n",
    "test_preds = bst.predict(dtest).astype(int) + 1   # back to labels 1–3\n",
    "\n",
    "submission_xgb = pd.DataFrame({\n",
    "    'id':   test_ids,\n",
    "    'y':    test_preds\n",
    "})\n",
    "submission_xgb.to_csv('xgb.csv', index=False)\n",
    "print(\"Saved xgb.csv with\", len(submission_xgb), \"rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
