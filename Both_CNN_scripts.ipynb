{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %pip install pandas numpy scikit-learn matplotlib seaborn tensorflow keras xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I tried a simple 2 layer CNN model, consisting of two convolutional blocks (3→32, 32→64) each with ReLU and max pooling, flattenning the resulting 64×8×8 feature map into a 4096 dimensional vector, and applies a single dropout layer in the fully connected head. Its design makes it fast and lightweight, but without batch normalization or extra layers it can’t learn as deepand well regularized features as a deeper network would.So I moved on to a 3layer CNN. This three layer CNN adds a third convolutional block (with 3→32→64→128 filters), each including batch normalization, activation, pooling, and dropout, to progressively learn richer image features down to a 128×4×4 map. It then flattens these 2048 features into a 128 unit dense layer (with dropout) before the final three way classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Read training and test ssets\u001b[39;00m\n",
      "\u001b[0;32m     12\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m---> 15\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     16\u001b[0m test_df  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_df\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n",
      "\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n",
      "\u001b[0;32m   1014\u001b[0m     dialect,\n",
      "\u001b[0;32m   1015\u001b[0m     delimiter,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n",
      "\u001b[0;32m   1023\u001b[0m )\n",
      "\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n",
      "\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n",
      "\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n",
      "\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n",
      "\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n",
      "\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n",
      "\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n",
      "\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n",
      "\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "#Read training and test ssets\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# Check for missing values\n",
    "assert train_df.isna().sum().sum() == 0, \"Missing in train!\"\n",
    "assert test_df .isna().sum().sum() == 0, \"Missing in test!\"\n",
    "\n",
    "# Separate IDs and labels\n",
    "train_ids = train_df[\"id\"].values\n",
    "## zero-based for PyTorch\n",
    "train_labels = train_df[\"y\"].values.astype(np.int64) - 1  \n",
    "train_df = train_df.drop(columns=[\"id\", \"y\"])\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_df = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "# Normalize pixel intensities(it is already in [0,1] but we cast to float32)\n",
    "train_pixels = train_df.values.astype(np.float32)\n",
    "test_pixels  = test_df.values .astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  Each row is [r3132, g0101, b0101], so we reshape to (N, 3, 32, 32)\n",
    "def to_image_array(flat_array):\n",
    "    # shape (N, 3072) → (N, 3, 32, 32)\n",
    "    return flat_array.reshape(-1, 3, 32, 32)\n",
    "\n",
    "X_train = to_image_array(train_pixels)\n",
    "X_test  = to_image_array(test_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FarmImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None):\n",
    "        self.images = torch.from_numpy(images)     \n",
    "        self.labels = None if labels is None else torch.from_numpy(labels)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]\n",
    "        if self.labels is None:\n",
    "            return x\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Split train into train/validation\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = FarmImageDataset(X_tr, y_tr)\n",
    "val_ds   = FarmImageDataset(X_val, y_val)\n",
    "test_ds  = FarmImageDataset(X_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "##simple CNN model\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),    #   → (32,32,32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                               #  → (32,16,16)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),    #  → (64,16,16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                               # →  (64,8,8)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                                  #  →  (64*8*8 = 4096)\n",
    "            nn.Linear(64*8*8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 – train_acc: 0.633, val_acc: 0.817\n",
      "Epoch 02 – train_acc: 0.768, val_acc: 0.838\n",
      "Epoch 03 – train_acc: 0.833, val_acc: 0.875\n",
      "Epoch 04 – train_acc: 0.874, val_acc: 0.887\n",
      "Epoch 05 – train_acc: 0.910, val_acc: 0.904\n",
      "Epoch 06 – train_acc: 0.903, val_acc: 0.908\n",
      "Epoch 07 – train_acc: 0.914, val_acc: 0.908\n",
      "Epoch 08 – train_acc: 0.922, val_acc: 0.921\n",
      "Epoch 09 – train_acc: 0.918, val_acc: 0.900\n",
      "Epoch 10 – train_acc: 0.894, val_acc: 0.904\n",
      "Epoch 11 – train_acc: 0.884, val_acc: 0.929\n",
      "Epoch 12 – train_acc: 0.941, val_acc: 0.946\n",
      "Epoch 13 – train_acc: 0.954, val_acc: 0.963\n",
      "Epoch 14 – train_acc: 0.963, val_acc: 0.954\n",
      "Epoch 15 – train_acc: 0.950, val_acc: 0.942\n",
      "Epoch 16 – train_acc: 0.948, val_acc: 0.921\n",
      "Epoch 17 – train_acc: 0.963, val_acc: 0.958\n",
      "Epoch 18 – train_acc: 0.967, val_acc: 0.933\n",
      "Epoch 19 – train_acc: 0.968, val_acc: 0.971\n",
      "Epoch 20 – train_acc: 0.974, val_acc: 0.933\n",
      "Epoch 21 – train_acc: 0.953, val_acc: 0.950\n",
      "Epoch 22 – train_acc: 0.974, val_acc: 0.946\n",
      "Epoch 23 – train_acc: 0.975, val_acc: 0.958\n",
      "Epoch 24 – train_acc: 0.978, val_acc: 0.958\n",
      "Epoch 25 – train_acc: 0.975, val_acc: 0.963\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        total_correct += (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "def eval_epoch(loader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)\n",
    "            total_loss += criterion(preds, yb).item() * Xb.size(0)\n",
    "            total_correct += (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(1, 26):\n",
    "    train_loss, train_acc = train_epoch(train_loader)\n",
    "    val_loss, val_acc     = eval_epoch(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_cnn.pth\")\n",
    "    print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved cnn_2.csv with 1200 rows.\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_cnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for Xb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        preds = model(Xb).argmax(1).cpu().numpy() + 1 \n",
    "        all_preds.append(preds)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "\n",
    "# generaate CSV\n",
    "submission_file = pd.DataFrame({\"id\": test_ids, \"y\": all_preds})\n",
    "submission_file.to_csv(\"cnn_2.csv\", index=False)\n",
    "print(\"saved cnn_2.csv with\", len(submission_file), \"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 – train_acc: 0.969, val_acc: 0.946\n",
      "Epoch 02 – train_acc: 0.969, val_acc: 0.942\n",
      "Epoch 03 – train_acc: 0.963, val_acc: 0.954\n",
      "Epoch 04 – train_acc: 0.957, val_acc: 0.954\n",
      "Epoch 05 – train_acc: 0.976, val_acc: 0.942\n",
      "Epoch 06 – train_acc: 0.975, val_acc: 0.967\n",
      "Epoch 07 – train_acc: 0.975, val_acc: 0.963\n",
      "Epoch 08 – train_acc: 0.974, val_acc: 0.967\n",
      "Epoch 09 – train_acc: 0.978, val_acc: 0.963\n",
      "Epoch 10 – train_acc: 0.981, val_acc: 0.942\n",
      "Epoch 11 – train_acc: 0.974, val_acc: 0.963\n",
      "Epoch 12 – train_acc: 0.983, val_acc: 0.938\n",
      "Epoch 13 – train_acc: 0.984, val_acc: 0.963\n",
      "Epoch 14 – train_acc: 0.985, val_acc: 0.929\n",
      "Epoch 15 – train_acc: 0.980, val_acc: 0.963\n",
      "Epoch 16 – train_acc: 0.983, val_acc: 0.946\n",
      "Epoch 17 – train_acc: 0.994, val_acc: 0.942\n",
      "Epoch 18 – train_acc: 0.989, val_acc: 0.967\n",
      "Epoch 19 – train_acc: 0.990, val_acc: 0.963\n",
      "Epoch 20 – train_acc: 0.994, val_acc: 0.942\n",
      "\n",
      "Best validation accuracy achieved: 96.67%\n",
      "Final validation accuracy (re-loaded best model): 96.67%\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "best_val_acc = 0\n",
    "for epoch in range(1, 21):\n",
    "    train_loss, train_acc = train_epoch(train_loader)\n",
    "    val_loss, val_acc     = eval_epoch(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_cnn.pth\")\n",
    "    print(f\"Epoch {epoch:02d} – train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nBest validation accuracy achieved: {best_val_acc*100:.2f}%\")\n",
    "# Load best weights and reevaluate\n",
    "model.load_state_dict(torch.load(\"best_cnn.pth\"))\n",
    "val_loss, val_acc = eval_epoch(val_loader)\n",
    "print(f\"Final validation accuracy (re-loaded best model): {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then moved on to a 3layer CNN model; This three layer CNN adds a third convolutional block (with 3→32→64→128 filters), each including batch normalization, activation, pooling, and dropout, to progressively learn richer image features down to a 128×4×4 map. It then flattens these 2048 features into a 128 unit dense layer (with dropout) before the final three way classification.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1200, 3074) Test: (1200, 3073)\n"
     ]
    }
   ],
   "source": [
    "#### 3 layer CNN:\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "#Read training and test ssets\n",
    "DATA_DIR = \"data\"\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# Check for missing values\n",
    "assert train_df.isna().sum().sum() == 0, \"Missing in train!\"\n",
    "assert test_df .isna().sum().sum() == 0, \"Missing in test!\"\n",
    "\n",
    "\n",
    "# Extract ids & labels\n",
    "train_ids    = train_df.pop(\"id\").values\n",
    "## zero-based for PyTorch\n",
    "train_labels = (train_df.pop(\"y\").values - 1).astype(np.int64)  \n",
    "test_ids     = test_df.pop(\"id\").values\n",
    "\n",
    "# Convert to float32 and reshape to (N,3,32,32)\n",
    "\n",
    "X_train = train_df.values.astype(np.float32).reshape(-1, 3, 32, 32)\n",
    "X_test  = test_df.values.astype(np.float32).reshape(-1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_acc: 0.647 | val_acc: 0.338\n",
      "Epoch 02 | train_acc: 0.883 | val_acc: 0.688\n",
      "Epoch 03 | train_acc: 0.920 | val_acc: 0.817\n",
      "Epoch 04 | train_acc: 0.934 | val_acc: 0.971\n",
      "Epoch 05 | train_acc: 0.954 | val_acc: 0.921\n",
      "Epoch 06 | train_acc: 0.941 | val_acc: 0.938\n",
      "Epoch 07 | train_acc: 0.948 | val_acc: 0.879\n",
      "Epoch 08 | train_acc: 0.948 | val_acc: 0.921\n",
      "Epoch 09 | train_acc: 0.946 | val_acc: 0.963\n",
      "Epoch 10 | train_acc: 0.954 | val_acc: 0.975\n",
      "Epoch 11 | train_acc: 0.970 | val_acc: 0.958\n",
      "Epoch 12 | train_acc: 0.967 | val_acc: 0.954\n",
      "Epoch 13 | train_acc: 0.959 | val_acc: 0.933\n",
      "Epoch 14 | train_acc: 0.977 | val_acc: 0.958\n",
      "Epoch 15 | train_acc: 0.974 | val_acc: 0.904\n",
      "Epoch 16 | train_acc: 0.983 | val_acc: 0.954\n",
      "Epoch 17 | train_acc: 0.978 | val_acc: 0.863\n",
      "Epoch 18 | train_acc: 0.942 | val_acc: 0.879\n",
      "Epoch 19 | train_acc: 0.971 | val_acc: 0.929\n",
      "Epoch 20 | train_acc: 0.984 | val_acc: 0.979\n",
      "Epoch 21 | train_acc: 0.984 | val_acc: 0.929\n",
      "Epoch 22 | train_acc: 0.990 | val_acc: 0.971\n",
      "Epoch 23 | train_acc: 0.991 | val_acc: 0.950\n",
      "Epoch 24 | train_acc: 0.978 | val_acc: 0.971\n",
      "Epoch 25 | train_acc: 0.981 | val_acc: 0.821\n",
      "Epoch 26 | train_acc: 0.982 | val_acc: 0.917\n",
      "Epoch 27 | train_acc: 0.989 | val_acc: 0.938\n",
      "Epoch 28 | train_acc: 0.984 | val_acc: 0.879\n",
      "Epoch 29 | train_acc: 0.983 | val_acc: 0.967\n",
      "Epoch 30 | train_acc: 0.988 | val_acc: 0.975\n",
      "\n",
      "Best validation accuracy: 97.92%\n",
      "Reloaded best model val_acc: 97.92%\n",
      "Saved 3L_cnn.csv with 1200 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DataLoader\n",
    "class FarmDataset(Dataset):\n",
    "    def __init__(self, images, labels=None):\n",
    "        self.images = torch.from_numpy(images)\n",
    "        self.labels = None if labels is None else torch.from_numpy(labels)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]\n",
    "        if self.labels is None:\n",
    "            return x\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Train/validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, train_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=train_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "batch_size   = 64\n",
    "train_loader = DataLoader(FarmDataset(X_tr,  y_tr), batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(FarmDataset(X_val, y_val), batch_size)\n",
    "test_loader  = DataLoader(FarmDataset(X_test),    batch_size)\n",
    "\n",
    "# Define the 3-layer CNN\n",
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 3→32\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),            ### 32×32×32 → 32×16×16\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Block 2: 32→64\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),              ##### 64×16×16 → 64×8×8\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Block 3: 64→128\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),            #### 128×8×8 → 128×4×4\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                  ####→ 128*4*4 = 2048\n",
    "            nn.Linear(2048, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# training setup\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = ThreeLayerCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = total_correct = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds  = model(xb)\n",
    "        loss   = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss   += loss.item() * xb.size(0)\n",
    "        total_correct+= (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "def eval_epoch(loader):\n",
    "    model.eval()\n",
    "    total_loss = total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds  = model(xb)\n",
    "            total_loss   += criterion(preds, yb).item() * xb.size(0)\n",
    "            total_correct+= (preds.argmax(1) == yb).sum().item()\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
    "\n",
    "#  Train for 30 epochs, save  the best model\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, 31):\n",
    "    tr_loss, tr_acc = train_epoch(train_loader)\n",
    "    vl_loss, vl_acc = eval_epoch(val_loader)\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save(model.state_dict(), \"best_three_layer_cnn.pth\")\n",
    "    print(f\"Epoch {epoch:02d} | train_acc: {tr_acc:.3f} | val_acc: {vl_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "# Reload best modeln and final validation\n",
    "model.load_state_dict(torch.load(\"best_three_layer_cnn.pth\"))\n",
    "_, final_acc = eval_epoch(val_loader)\n",
    "print(f\"Reloaded best model val_acc: {final_acc*100:.2f}%\")\n",
    "\n",
    "# generate CSV\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb    = xb.to(device)\n",
    "        preds = model(xb).argmax(1).cpu().numpy() + 1  \n",
    "        all_preds.append(preds)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"y\":  all_preds\n",
    "})\n",
    "submission.to_csv(\"3L_cnn.csv\", index=False)\n",
    "print(\"Saved 3L_cnn.csv with\", len(submission), \"rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
